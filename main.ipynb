{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduction includes a brief description of your and what problem your\n",
    "technique would be applied to\n",
    "- Introduction includes a description of the dataset that you used\n",
    "- Introduction includes and the specific inputs and outputs of the problem\n",
    "- Introduction clearly states the two techniques used (see section below)\n",
    "- Introduction clearly states the experimental question explored in this\n",
    "project (see experiment section below)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify \n",
    "Is a Hand-Drawn Image to Emoji classification ML system that uses Convolutional \n",
    "Neural Networks. \n",
    "\n",
    "We used the drawn images from Google's \"Quick, Draw!\" dataset and labeled the output text with emoji's from https://emojipedia.org/.\n",
    "\n",
    "Our system takes in a drawn image and outputs an emoji.\n",
    "\n",
    "The two techniques we used were MobileNetv2 and \n",
    "\n",
    "Our experimental question is"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  6889  100  6889    0     0  20020      0 --:--:-- --:--:-- --:--:-- 20381\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1207  100  1207    0     0   4945      0 --:--:-- --:--:-- --:--:--  4987- --:--:-- --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# curl csv file from google drive\n",
    "!curl \"https://raw.githubusercontent.com/victorhuangwq/Emojify/main/data/text-to-emoji.csv\" -o \"dataset/text-to-emoji.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-macos\n",
      "  Using cached tensorflow_macos-2.12.0-cp38-cp38-macosx_12_0_arm64.whl (200.8 MB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-macos)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-macos)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=2.0 (from tensorflow-macos)\n",
      "  Using cached flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-macos)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-macos)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-macos)\n",
      "  Using cached h5py-3.8.0-cp38-cp38-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-macos)\n",
      "  Using cached jax-0.4.8.tar.gz (1.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting libclang>=13.0.0 (from tensorflow-macos)\n",
      "  Using cached libclang-16.0.0-py2.py3-none-macosx_11_0_arm64.whl (24.3 MB)\n",
      "Collecting numpy<1.24,>=1.22 (from tensorflow-macos)\n",
      "  Using cached numpy-1.23.5-cp38-cp38-macosx_11_0_arm64.whl (13.3 MB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-macos)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.8/site-packages (from tensorflow-macos) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-macos)\n",
      "  Using cached protobuf-4.22.3-cp37-abi3-macosx_10_9_universal2.whl (397 kB)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from tensorflow-macos) (56.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.8/site-packages (from tensorflow-macos) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-macos)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.8/site-packages (from tensorflow-macos) (4.5.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-macos)\n",
      "  Using cached wrapt-1.14.1-cp38-cp38-macosx_11_0_arm64.whl (35 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-macos)\n",
      "  Using cached grpcio-1.54.0-cp38-cp38-macosx_10_10_universal2.whl (8.7 MB)\n",
      "Collecting tensorboard<2.13,>=2.12 (from tensorflow-macos)\n",
      "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow-macos)\n",
      "  Using cached tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting keras<2.13,>=2.12.0 (from tensorflow-macos)\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-macos)\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting ml-dtypes>=0.0.3 (from jax>=0.3.15->tensorflow-macos)\n",
      "  Using cached ml_dtypes-0.1.0-cp38-cp38-macosx_10_9_universal2.whl (317 kB)\n",
      "Collecting scipy>=1.7 (from jax>=0.3.15->tensorflow-macos)\n",
      "  Using cached scipy-1.10.1-cp38-cp38-macosx_12_0_arm64.whl (28.8 MB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached requests-2.30.0-py3-none-any.whl (62 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached tensorboard_data_server-0.7.0-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached Werkzeug-2.3.3-py3-none-any.whl (242 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./venv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-macos) (6.6.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached charset_normalizer-3.1.0-cp38-cp38-macosx_11_0_arm64.whl (121 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached MarkupSafe-2.1.2-cp38-cp38-macosx_10_9_universal2.whl (17 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-macos) (3.15.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.4.8-py3-none-any.whl size=1439678 sha256=367f0a90854383e18dcf8fb9406f5fce4e0a06e54e701d81383a5f488fc4790c\n",
      "  Stored in directory: /Users/victorhuang/Library/Caches/pip/wheels/45/83/1e/3db22c5e1941c10e41c4f5cdf829b0a358146d4d0733d4a105\n",
      "Successfully built jax\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, termcolor, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, MarkupSafe, keras, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, scipy, rsa, requests, pyasn1-modules, opt-einsum, ml-dtypes, markdown, h5py, astunparse, requests-oauthlib, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow-macos\n",
      "Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 certifi-2022.12.7 charset-normalizer-3.1.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.17.3 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.0 h5py-3.8.0 idna-3.4 jax-0.4.8 keras-2.12.0 libclang-16.0.0 markdown-3.4.3 ml-dtypes-0.1.0 numpy-1.23.5 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.22.3 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.30.0 requests-oauthlib-1.3.1 rsa-4.9 scipy-1.10.1 tensorboard-2.12.3 tensorboard-data-server-0.7.0 tensorflow-estimator-2.12.0 tensorflow-macos-2.12.0 termcolor-2.3.0 urllib3-2.0.2 werkzeug-2.3.3 wheel-0.40.0 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-macos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./venv/lib/python3.8/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.8/site-packages (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.8/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in ./venv/lib/python3.8/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py in ./venv/lib/python3.8/site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Collecting array-record (from tensorflow_datasets)\n",
      "  Downloading array_record-0.2.0-py38-none-any.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting click (from tensorflow_datasets)\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.8-cp38-cp38-macosx_11_0_arm64.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting etils[enp,epath]>=0.9.0 (from tensorflow_datasets)\n",
      "  Downloading etils-1.2.0-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./venv/lib/python3.8/site-packages (from tensorflow_datasets) (1.23.5)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in ./venv/lib/python3.8/site-packages (from tensorflow_datasets) (4.22.3)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.8/site-packages (from tensorflow_datasets) (5.9.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./venv/lib/python3.8/site-packages (from tensorflow_datasets) (2.30.0)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: termcolor in ./venv/lib/python3.8/site-packages (from tensorflow_datasets) (2.3.0)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tqdm (from tensorflow_datasets)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in ./venv/lib/python3.8/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: importlib-resources in ./venv/lib/python3.8/site-packages (from tensorflow_datasets) (5.12.0)\n",
      "Requirement already satisfied: typing_extensions in ./venv/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (4.5.0)\n",
      "Requirement already satisfied: zipp in ./venv/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2022.12.7)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.8/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=79ec1a881373c687a3ab1db3e9ddb255450b78afe01f42970bb7b95e4f0143c5\n",
      "  Stored in directory: /Users/victorhuang/Library/Caches/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, tqdm, toml, promise, googleapis-common-protos, etils, click, tensorflow-metadata, array-record, tensorflow_datasets\n",
      "Successfully installed array-record-0.2.0 click-8.1.3 dm-tree-0.1.8 etils-1.2.0 googleapis-common-protos-1.59.0 promise-2.3 tensorflow-metadata-1.13.1 tensorflow_datasets-4.9.2 toml-0.10.2 tqdm-4.65.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorhuang/code/Emojify/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 11:08:40.478004: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 36.82 GiB (download: 36.82 GiB, generated: Unknown size, total: 36.82 GiB) to /Users/victorhuang/tensorflow_datasets/quickdraw_bitmap/3.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...:  10%|▉         | 507/5218 [00:24<03:49, 20.53 MiB/s]\n",
      "Dl Completed...:   0%|          | 0/345 [00:24<?, ? url/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# import quickdraw dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m quickdraw_train \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mquickdraw_bitmap\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, shuffle_files\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/tensorflow_datasets/core/load.py:640\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \n\u001b[1;32m    523\u001b[0m \u001b[39m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[39m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m dbuilder \u001b[39m=\u001b[39m _fetch_builder(\n\u001b[1;32m    635\u001b[0m     name,\n\u001b[1;32m    636\u001b[0m     data_dir,\n\u001b[1;32m    637\u001b[0m     builder_kwargs,\n\u001b[1;32m    638\u001b[0m     try_gcs,\n\u001b[1;32m    639\u001b[0m )\n\u001b[0;32m--> 640\u001b[0m _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)\n\u001b[1;32m    642\u001b[0m \u001b[39mif\u001b[39;00m as_dataset_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    643\u001b[0m   as_dataset_kwargs \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/tensorflow_datasets/core/load.py:499\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[0;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[1;32m    498\u001b[0m   download_and_prepare_kwargs \u001b[39m=\u001b[39m download_and_prepare_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m--> 499\u001b[0m   dbuilder\u001b[39m.\u001b[39;49mdownload_and_prepare(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs)\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py:646\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[1;32m    644\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mread_from_directory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_dir)\n\u001b[1;32m    645\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 646\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    647\u001b[0m       dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    648\u001b[0m       download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m    649\u001b[0m   )\n\u001b[1;32m    651\u001b[0m   \u001b[39m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[1;32m    652\u001b[0m   \u001b[39m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[1;32m    653\u001b[0m   \u001b[39m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[1;32m    654\u001b[0m   \u001b[39m# when reading from package data.\u001b[39;00m\n\u001b[1;32m    655\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdownload_size \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mdownloaded_size\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py:1498\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1497\u001b[0m   optional_pipeline_kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1498\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_split_generators(  \u001b[39m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[1;32m   1499\u001b[0m     dl_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptional_pipeline_kwargs\n\u001b[1;32m   1500\u001b[0m )\n\u001b[1;32m   1501\u001b[0m \u001b[39m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[39m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[39m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m split_generators \u001b[39m=\u001b[39m split_builder\u001b[39m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[1;32m   1505\u001b[0m     split_generators\u001b[39m=\u001b[39msplit_generators,\n\u001b[1;32m   1506\u001b[0m     generator_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_examples,\n\u001b[1;32m   1507\u001b[0m     is_beam\u001b[39m=\u001b[39m\u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, BeamBasedBuilder),\n\u001b[1;32m   1508\u001b[0m )\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/tensorflow_datasets/datasets/quickdraw_bitmap/quickdraw_bitmap_dataset_builder.py:65\u001b[0m, in \u001b[0;36mBuilder._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     59\u001b[0m labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mnames\n\u001b[1;32m     60\u001b[0m urls \u001b[39m=\u001b[39m {\n\u001b[1;32m     61\u001b[0m     label: \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(_QUICKDRAW_BASE_URL, label)\n\u001b[1;32m     62\u001b[0m     \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m labels\n\u001b[1;32m     63\u001b[0m }\n\u001b[0;32m---> 65\u001b[0m file_paths \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39;49mdownload(urls)\n\u001b[1;32m     67\u001b[0m \u001b[39m# There is no predefined train/test split for this dataset.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m     69\u001b[0m     tfds\u001b[39m.\u001b[39mcore\u001b[39m.\u001b[39mSplitGenerator(\n\u001b[1;32m     70\u001b[0m         name\u001b[39m=\u001b[39mtfds\u001b[39m.\u001b[39mSplit\u001b[39m.\u001b[39mTRAIN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     75\u001b[0m ]\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/tensorflow_datasets/core/download/download_manager.py:600\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Add progress bar to follow the download state\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_downloader\u001b[39m.\u001b[39mtqdm():\n\u001b[0;32m--> 600\u001b[0m   \u001b[39mreturn\u001b[39;00m _map_promise(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download, url_or_urls)\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/tensorflow_datasets/core/download/download_manager.py:830\u001b[0m, in \u001b[0;36m_map_promise\u001b[0;34m(map_fn, all_inputs)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[1;32m    827\u001b[0m all_promises \u001b[39m=\u001b[39m tree_utils\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m    828\u001b[0m     map_fn, all_inputs\n\u001b[1;32m    829\u001b[0m )  \u001b[39m# Apply the function\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m res \u001b[39m=\u001b[39m tree_utils\u001b[39m.\u001b[39;49mmap_structure(\n\u001b[1;32m    831\u001b[0m     \u001b[39mlambda\u001b[39;49;00m p: p\u001b[39m.\u001b[39;49mget(), all_promises\n\u001b[1;32m    832\u001b[0m )  \u001b[39m# Wait promises\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39mfor\u001b[39;00m other \u001b[39min\u001b[39;00m structures[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[39m0\u001b[39m], other, check_types\u001b[39m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m unflatten_as(structures[\u001b[39m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [func(\u001b[39m*\u001b[39margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39mfor\u001b[39;00m other \u001b[39min\u001b[39;00m structures[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[39m0\u001b[39m], other, check_types\u001b[39m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m unflatten_as(structures[\u001b[39m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [func(\u001b[39m*\u001b[39;49margs) \u001b[39mfor\u001b[39;00m args \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/tensorflow_datasets/core/download/download_manager.py:831\u001b[0m, in \u001b[0;36m_map_promise.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[1;32m    827\u001b[0m all_promises \u001b[39m=\u001b[39m tree_utils\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m    828\u001b[0m     map_fn, all_inputs\n\u001b[1;32m    829\u001b[0m )  \u001b[39m# Apply the function\u001b[39;00m\n\u001b[1;32m    830\u001b[0m res \u001b[39m=\u001b[39m tree_utils\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m--> 831\u001b[0m     \u001b[39mlambda\u001b[39;00m p: p\u001b[39m.\u001b[39;49mget(), all_promises\n\u001b[1;32m    832\u001b[0m )  \u001b[39m# Wait promises\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/promise/promise.py:511\u001b[0m, in \u001b[0;36mPromise.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    509\u001b[0m     \u001b[39m# type: (Optional[float]) -> T\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target()\n\u001b[0;32m--> 511\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout \u001b[39mor\u001b[39;49;00m DEFAULT_TIMEOUT)\n\u001b[1;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_settled_value(_raise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/promise/promise.py:506\u001b[0m, in \u001b[0;36mPromise._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    505\u001b[0m     \u001b[39m# type: (Optional[float]) -> None\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(\u001b[39mself\u001b[39;49m, timeout)\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/promise/promise.py:502\u001b[0m, in \u001b[0;36mPromise.wait\u001b[0;34m(cls, promise, timeout)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    500\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mcls\u001b[39m, promise, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    501\u001b[0m     \u001b[39m# type: (Promise, Optional[float]) -> None\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     async_instance\u001b[39m.\u001b[39;49mwait(promise, timeout)\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/promise/async_.py:117\u001b[0m, in \u001b[0;36mAsync.wait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m promise\u001b[39m.\u001b[39mis_pending:\n\u001b[1;32m    114\u001b[0m         \u001b[39m# We return if the promise is already\u001b[39;00m\n\u001b[1;32m    115\u001b[0m         \u001b[39m# fulfilled or rejected\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m target\u001b[39m.\u001b[39;49mscheduler\u001b[39m.\u001b[39;49mwait(target, timeout)\n",
      "File \u001b[0;32m~/code/Emojify/venv/lib/python3.8/site-packages/promise/schedulers/immediate.py:25\u001b[0m, in \u001b[0;36mImmediateScheduler.wait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m     22\u001b[0m     e\u001b[39m.\u001b[39mset()\n\u001b[1;32m     24\u001b[0m promise\u001b[39m.\u001b[39m_then(on_resolve_or_reject, on_resolve_or_reject)\n\u001b[0;32m---> 25\u001b[0m waited \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m waited:\n\u001b[1;32m     27\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTimeout\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import quickdraw dataset\n",
    "quickdraw_train = tfds.load('quickdraw_bitmap', split='train', shuffle_files=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_4\n",
      "1 conv2d_282\n",
      "2 batch_normalization_282\n",
      "3 activation_282\n",
      "4 conv2d_283\n",
      "5 batch_normalization_283\n",
      "6 activation_283\n",
      "7 conv2d_284\n",
      "8 batch_normalization_284\n",
      "9 activation_284\n",
      "10 max_pooling2d_12\n",
      "11 conv2d_285\n",
      "12 batch_normalization_285\n",
      "13 activation_285\n",
      "14 conv2d_286\n",
      "15 batch_normalization_286\n",
      "16 activation_286\n",
      "17 max_pooling2d_13\n",
      "18 conv2d_290\n",
      "19 batch_normalization_290\n",
      "20 activation_290\n",
      "21 conv2d_288\n",
      "22 conv2d_291\n",
      "23 batch_normalization_288\n",
      "24 batch_normalization_291\n",
      "25 activation_288\n",
      "26 activation_291\n",
      "27 average_pooling2d_27\n",
      "28 conv2d_287\n",
      "29 conv2d_289\n",
      "30 conv2d_292\n",
      "31 conv2d_293\n",
      "32 batch_normalization_287\n",
      "33 batch_normalization_289\n",
      "34 batch_normalization_292\n",
      "35 batch_normalization_293\n",
      "36 activation_287\n",
      "37 activation_289\n",
      "38 activation_292\n",
      "39 activation_293\n",
      "40 mixed0\n",
      "41 conv2d_297\n",
      "42 batch_normalization_297\n",
      "43 activation_297\n",
      "44 conv2d_295\n",
      "45 conv2d_298\n",
      "46 batch_normalization_295\n",
      "47 batch_normalization_298\n",
      "48 activation_295\n",
      "49 activation_298\n",
      "50 average_pooling2d_28\n",
      "51 conv2d_294\n",
      "52 conv2d_296\n",
      "53 conv2d_299\n",
      "54 conv2d_300\n",
      "55 batch_normalization_294\n",
      "56 batch_normalization_296\n",
      "57 batch_normalization_299\n",
      "58 batch_normalization_300\n",
      "59 activation_294\n",
      "60 activation_296\n",
      "61 activation_299\n",
      "62 activation_300\n",
      "63 mixed1\n",
      "64 conv2d_304\n",
      "65 batch_normalization_304\n",
      "66 activation_304\n",
      "67 conv2d_302\n",
      "68 conv2d_305\n",
      "69 batch_normalization_302\n",
      "70 batch_normalization_305\n",
      "71 activation_302\n",
      "72 activation_305\n",
      "73 average_pooling2d_29\n",
      "74 conv2d_301\n",
      "75 conv2d_303\n",
      "76 conv2d_306\n",
      "77 conv2d_307\n",
      "78 batch_normalization_301\n",
      "79 batch_normalization_303\n",
      "80 batch_normalization_306\n",
      "81 batch_normalization_307\n",
      "82 activation_301\n",
      "83 activation_303\n",
      "84 activation_306\n",
      "85 activation_307\n",
      "86 mixed2\n",
      "87 conv2d_309\n",
      "88 batch_normalization_309\n",
      "89 activation_309\n",
      "90 conv2d_310\n",
      "91 batch_normalization_310\n",
      "92 activation_310\n",
      "93 conv2d_308\n",
      "94 conv2d_311\n",
      "95 batch_normalization_308\n",
      "96 batch_normalization_311\n",
      "97 activation_308\n",
      "98 activation_311\n",
      "99 max_pooling2d_14\n",
      "100 mixed3\n",
      "101 conv2d_316\n",
      "102 batch_normalization_316\n",
      "103 activation_316\n",
      "104 conv2d_317\n",
      "105 batch_normalization_317\n",
      "106 activation_317\n",
      "107 conv2d_313\n",
      "108 conv2d_318\n",
      "109 batch_normalization_313\n",
      "110 batch_normalization_318\n",
      "111 activation_313\n",
      "112 activation_318\n",
      "113 conv2d_314\n",
      "114 conv2d_319\n",
      "115 batch_normalization_314\n",
      "116 batch_normalization_319\n",
      "117 activation_314\n",
      "118 activation_319\n",
      "119 average_pooling2d_30\n",
      "120 conv2d_312\n",
      "121 conv2d_315\n",
      "122 conv2d_320\n",
      "123 conv2d_321\n",
      "124 batch_normalization_312\n",
      "125 batch_normalization_315\n",
      "126 batch_normalization_320\n",
      "127 batch_normalization_321\n",
      "128 activation_312\n",
      "129 activation_315\n",
      "130 activation_320\n",
      "131 activation_321\n",
      "132 mixed4\n",
      "133 conv2d_326\n",
      "134 batch_normalization_326\n",
      "135 activation_326\n",
      "136 conv2d_327\n",
      "137 batch_normalization_327\n",
      "138 activation_327\n",
      "139 conv2d_323\n",
      "140 conv2d_328\n",
      "141 batch_normalization_323\n",
      "142 batch_normalization_328\n",
      "143 activation_323\n",
      "144 activation_328\n",
      "145 conv2d_324\n",
      "146 conv2d_329\n",
      "147 batch_normalization_324\n",
      "148 batch_normalization_329\n",
      "149 activation_324\n",
      "150 activation_329\n",
      "151 average_pooling2d_31\n",
      "152 conv2d_322\n",
      "153 conv2d_325\n",
      "154 conv2d_330\n",
      "155 conv2d_331\n",
      "156 batch_normalization_322\n",
      "157 batch_normalization_325\n",
      "158 batch_normalization_330\n",
      "159 batch_normalization_331\n",
      "160 activation_322\n",
      "161 activation_325\n",
      "162 activation_330\n",
      "163 activation_331\n",
      "164 mixed5\n",
      "165 conv2d_336\n",
      "166 batch_normalization_336\n",
      "167 activation_336\n",
      "168 conv2d_337\n",
      "169 batch_normalization_337\n",
      "170 activation_337\n",
      "171 conv2d_333\n",
      "172 conv2d_338\n",
      "173 batch_normalization_333\n",
      "174 batch_normalization_338\n",
      "175 activation_333\n",
      "176 activation_338\n",
      "177 conv2d_334\n",
      "178 conv2d_339\n",
      "179 batch_normalization_334\n",
      "180 batch_normalization_339\n",
      "181 activation_334\n",
      "182 activation_339\n",
      "183 average_pooling2d_32\n",
      "184 conv2d_332\n",
      "185 conv2d_335\n",
      "186 conv2d_340\n",
      "187 conv2d_341\n",
      "188 batch_normalization_332\n",
      "189 batch_normalization_335\n",
      "190 batch_normalization_340\n",
      "191 batch_normalization_341\n",
      "192 activation_332\n",
      "193 activation_335\n",
      "194 activation_340\n",
      "195 activation_341\n",
      "196 mixed6\n",
      "197 conv2d_346\n",
      "198 batch_normalization_346\n",
      "199 activation_346\n",
      "200 conv2d_347\n",
      "201 batch_normalization_347\n",
      "202 activation_347\n",
      "203 conv2d_343\n",
      "204 conv2d_348\n",
      "205 batch_normalization_343\n",
      "206 batch_normalization_348\n",
      "207 activation_343\n",
      "208 activation_348\n",
      "209 conv2d_344\n",
      "210 conv2d_349\n",
      "211 batch_normalization_344\n",
      "212 batch_normalization_349\n",
      "213 activation_344\n",
      "214 activation_349\n",
      "215 average_pooling2d_33\n",
      "216 conv2d_342\n",
      "217 conv2d_345\n",
      "218 conv2d_350\n",
      "219 conv2d_351\n",
      "220 batch_normalization_342\n",
      "221 batch_normalization_345\n",
      "222 batch_normalization_350\n",
      "223 batch_normalization_351\n",
      "224 activation_342\n",
      "225 activation_345\n",
      "226 activation_350\n",
      "227 activation_351\n",
      "228 mixed7\n",
      "229 conv2d_354\n",
      "230 batch_normalization_354\n",
      "231 activation_354\n",
      "232 conv2d_355\n",
      "233 batch_normalization_355\n",
      "234 activation_355\n",
      "235 conv2d_352\n",
      "236 conv2d_356\n",
      "237 batch_normalization_352\n",
      "238 batch_normalization_356\n",
      "239 activation_352\n",
      "240 activation_356\n",
      "241 conv2d_353\n",
      "242 conv2d_357\n",
      "243 batch_normalization_353\n",
      "244 batch_normalization_357\n",
      "245 activation_353\n",
      "246 activation_357\n",
      "247 max_pooling2d_15\n",
      "248 mixed8\n",
      "249 conv2d_362\n",
      "250 batch_normalization_362\n",
      "251 activation_362\n",
      "252 conv2d_359\n",
      "253 conv2d_363\n",
      "254 batch_normalization_359\n",
      "255 batch_normalization_363\n",
      "256 activation_359\n",
      "257 activation_363\n",
      "258 conv2d_360\n",
      "259 conv2d_361\n",
      "260 conv2d_364\n",
      "261 conv2d_365\n",
      "262 average_pooling2d_34\n",
      "263 conv2d_358\n",
      "264 batch_normalization_360\n",
      "265 batch_normalization_361\n",
      "266 batch_normalization_364\n",
      "267 batch_normalization_365\n",
      "268 conv2d_366\n",
      "269 batch_normalization_358\n",
      "270 activation_360\n",
      "271 activation_361\n",
      "272 activation_364\n",
      "273 activation_365\n",
      "274 batch_normalization_366\n",
      "275 activation_358\n",
      "276 mixed9_0\n",
      "277 concatenate_6\n",
      "278 activation_366\n",
      "279 mixed9\n",
      "280 conv2d_371\n",
      "281 batch_normalization_371\n",
      "282 activation_371\n",
      "283 conv2d_368\n",
      "284 conv2d_372\n",
      "285 batch_normalization_368\n",
      "286 batch_normalization_372\n",
      "287 activation_368\n",
      "288 activation_372\n",
      "289 conv2d_369\n",
      "290 conv2d_370\n",
      "291 conv2d_373\n",
      "292 conv2d_374\n",
      "293 average_pooling2d_35\n",
      "294 conv2d_367\n",
      "295 batch_normalization_369\n",
      "296 batch_normalization_370\n",
      "297 batch_normalization_373\n",
      "298 batch_normalization_374\n",
      "299 conv2d_375\n",
      "300 batch_normalization_367\n",
      "301 activation_369\n",
      "302 activation_370\n",
      "303 activation_373\n",
      "304 activation_374\n",
      "305 batch_normalization_375\n",
      "306 activation_367\n",
      "307 mixed9_1\n",
      "308 concatenate_7\n",
      "309 activation_375\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(200, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit(...)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
